import { NextRequest, NextResponse } from 'next/server';
import { NvidiaModelClient } from '@/lib/nvidia-models-simple';

// POST /api/nvidia/chat - NVIDIAå¤šæ¨¡å‹èŠå¤©API
export async function POST(request: NextRequest) {
  try {
    const body: any = await request.json();
    const { 
      message,
      messages, 
      model = 'auto', 
      taskType = 'conversation',
      temperature = 0.7,
      maxTokens = 4096,
      enableThinking = true
    } = body;

    // æ”¯æŒå•ä¸ªæ¶ˆæ¯æˆ–æ¶ˆæ¯æ•°ç»„
    let messageArray;
    if (message && typeof message === 'string') {
      messageArray = [{ role: 'user', content: message }];
    } else if (messages && Array.isArray(messages)) {
      messageArray = messages;
    } else {
      return NextResponse.json(
        { success: false, error: 'Message or messages array is required' },
        { status: 400 }
      );
    }

    console.log(`ğŸš€ NVIDIAå¤šæ¨¡å‹è°ƒç”¨ - æ¨¡å‹: ${model}, ä»»åŠ¡ç±»å‹: ${taskType}`);

    try {
      const client = new NvidiaModelClient();
      let response;

      if (model === 'glm4.7') {
        response = await client.callGLM47(messageArray, {
          temperature,
          maxTokens,
          enableThinking
        });
      } else if (model === 'kimi2.5') {
        response = await client.callKimi25(messageArray, {
          temperature,
          maxTokens,
          thinking: enableThinking
        });
      } else {
        response = await client.smartCall(messageArray, taskType, {
          temperature,
          maxTokens
        });
      }

      const assistantMessage = response.choices?.[0]?.message;
      const reasoning = assistantMessage?.reasoning_content;

      console.log(`âœ… NVIDIA APIè°ƒç”¨æˆåŠŸ - æ¨¡å‹: ${response.model || model}`);
      if (reasoning) {
        console.log(`ğŸ§  æ¨ç†è¿‡ç¨‹: ${reasoning.substring(0, 100)}...`);
      }

      return NextResponse.json({
        success: true,
        data: {
          id: response.id || Date.now().toString(),
          model: response.model || model,
          content: assistantMessage?.content || 'æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚',
          reasoning: reasoning || null,
          usage: response.usage,
          timestamp: new Date().toISOString()
        }
      });

    } catch (apiError) {
      console.error('âŒ NVIDIA APIè°ƒç”¨å¤±è´¥:', apiError);
      
      // æä¾›é™çº§å“åº”
      return NextResponse.json({
        success: true,
        data: {
          id: Date.now().toString(),
          model: model,
          content: `æˆ‘æ˜¯é£˜å”AIåŠ©æ‰‹ã€‚å…³äºæ‚¨çš„é—®é¢˜"${message || messageArray[messageArray.length - 1]?.content}"ï¼Œæˆ‘ä¼šå°½åŠ›ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚`,
          reasoning: 'NVIDIA APIæš‚æ—¶ä¸å¯ç”¨ï¼Œä½¿ç”¨é™çº§å“åº”',
          usage: null,
          timestamp: new Date().toISOString()
        }
      });
    }

  } catch (error) {
    console.error('âŒ NVIDIAèŠå¤©APIå¤„ç†å¤±è´¥:', error);
    
    return NextResponse.json(
      { 
        success: false, 
        error: error instanceof Error ? error.message : 'Unknown error',
        timestamp: new Date().toISOString()
      },
      { status: 500 }
    );
  }
}

// GET /api/nvidia/chat - è·å–å¯ç”¨æ¨¡å‹ä¿¡æ¯
export async function GET() {
  try {
    const client = new NvidiaModelClient();
    const models = client.getAvailableModels();
    const isConnected = await client.checkConnection();

    return NextResponse.json({
      success: true,
      data: {
        models,
        connected: isConnected,
        apiKey: process.env.NVIDIA_API_KEY ? 'å·²é…ç½®' : 'æœªé…ç½®',
        username: process.env.NVIDIA_USERNAME || 'æœªè®¾ç½®'
      }
    });

  } catch (error) {
    console.error('âŒ è·å–NVIDIAæ¨¡å‹ä¿¡æ¯å¤±è´¥:', error);
    
    return NextResponse.json(
      { 
        success: false, 
        error: error instanceof Error ? error.message : 'Unknown error'
      },
      { status: 500 }
    );
  }
}