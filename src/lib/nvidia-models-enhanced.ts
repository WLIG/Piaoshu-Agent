// NVIDIA Build API å¢å¼ºç‰ˆæœ¬ - é›†æˆGLM4.7ã€Kimi2.5å’ŒNemotron

interface Message {
  role: string;
  content: string;
}

interface CallOptions {
  temperature?: number;
  maxTokens?: number;
  enableThinking?: boolean;
  thinking?: boolean;
  stream?: boolean;
  reasoningBudget?: number;
}

interface StreamChunk {
  choices?: Array<{
    delta?: {
      content?: string;
      reasoning_content?: string;
    };
  }>;
}

export class NvidiaModelClient {
  private apiKey: string;
  private baseUrl: string;
  private username: string;

  constructor() {
    this.apiKey = process.env.NVIDIA_API_KEY || '';
    this.baseUrl = process.env.NVIDIA_BASE_URL || 'https://integrate.api.nvidia.com/v1';
    this.username = process.env.NVIDIA_USERNAME || 'NVIDIABuild-Autogen-37';
  }

  // GLM4.7 è°ƒç”¨ - æ€ç»´é“¾æ¨ç†ä¸“å®¶
  async callGLM47(messages: Message[], options: CallOptions = {}): Promise<any> {
    const {
      temperature = 1,
      maxTokens = 16384,
      enableThinking = true,
      stream = false
    } = options;

    const payload = {
      model: 'z-ai/glm4.7',
      messages,
      temperature,
      top_p: 1,
      max_tokens: maxTokens,
      stream,
      extra_body: {
        chat_template_kwargs: {
          enable_thinking: enableThinking,
          clear_thinking: false
        }
      }
    };

    return this.makeRequest(payload, stream, 'GLM4.7');
  }

  // Kimi2.5 è°ƒç”¨ - åˆ›æ„ç”Ÿæˆå¤§å¸ˆ
  async callKimi25(messages: Message[], options: CallOptions = {}): Promise<any> {
    const {
      temperature = 1.00,
      maxTokens = 16384,
      thinking = true,
      stream = false
    } = options;

    const payload = {
      model: 'moonshotai/kimi-k2.5',
      messages,
      temperature,
      top_p: 1.00,
      max_tokens: maxTokens,
      stream,
      chat_template_kwargs: {
        thinking
      }
    };

    return this.makeRequest(payload, stream, 'Kimi2.5');
  }

  // Nemotron è°ƒç”¨ - ä¸“ä¸šæ¨ç†åˆ†æ
  async callNemotron(messages: Message[], options: CallOptions = {}): Promise<any> {
    const {
      temperature = 0.8,
      maxTokens = 2048,
      enableThinking = true,
      reasoningBudget = 1024,
      stream = false
    } = options;

    const payload = {
      model: 'nvidia/nemotron-3-nano-30b-a3b',
      messages,
      temperature,
      top_p: 1,
      max_tokens: maxTokens,
      stream,
      extra_body: {
        reasoning_budget: reasoningBudget,
        chat_template_kwargs: {
          enable_thinking: enableThinking
        }
      }
    };

    return this.makeRequest(payload, stream, 'Nemotron');
  }

  // ç»Ÿä¸€è¯·æ±‚å¤„ç†
  private async makeRequest(payload: any, stream: boolean, modelName: string): Promise<any> {
    const headers: { [key: string]: string } = {
      'Authorization': `Bearer ${this.apiKey}`,
      'Content-Type': 'application/json'
    };

    if (stream) {
      headers['Accept'] = 'text/event-stream';
    }

    const response = await fetch(`${this.baseUrl}/chat/completions`, {
      method: 'POST',
      headers,
      body: JSON.stringify(payload)
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`${modelName} API error: ${response.status} - ${errorText}`);
    }

    if (stream) {
      return this.handleStreamResponse(response);
    }

    return await response.json();
  }

  // å¤„ç†æµå¼å“åº”
  private async handleStreamResponse(response: Response): Promise<any> {
    const reader = response.body?.getReader();
    if (!reader) {
      throw new Error('No response body reader available');
    }

    const decoder = new TextDecoder();
    let result = '';
    let reasoning = '';

    try {
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value);
        const lines = chunk.split('\n');

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6);
            if (data === '[DONE]') continue;

            try {
              const parsed = JSON.parse(data) as StreamChunk;
              const delta = parsed.choices?.[0]?.delta;
              
              if (delta?.reasoning_content) {
                reasoning += delta.reasoning_content;
              }
              
              if (delta?.content) {
                result += delta.content;
              }
            } catch (e) {
              // å¿½ç•¥è§£æé”™è¯¯
            }
          }
        }
      }
    } finally {
      reader.releaseLock();
    }

    return {
      choices: [{
        message: {
          content: result,
          reasoning_content: reasoning
        }
      }]
    };
  }

  // æ™ºèƒ½æ¨¡å‹é€‰æ‹© - å¢å¼ºç‰ˆ
  async smartCall(messages: Message[], taskType: string = 'conversation', options: CallOptions = {}): Promise<any> {
    const messageLength = messages.reduce((sum: number, msg: Message) => sum + (msg.content?.length || 0), 0);
    
    // å¤æ‚æ¨ç†ä»»åŠ¡ - ä½¿ç”¨Nemotron
    if (taskType === 'reasoning' || taskType === 'analysis' || messageLength > 10000) {
      console.log('ğŸ§  ä½¿ç”¨ Nemotron è¿›è¡Œæ·±åº¦æ¨ç†åˆ†æ');
      return this.callNemotron(messages, { enableThinking: true, ...options });
    }
    // åˆ›æ„ä»»åŠ¡ - ä½¿ç”¨Kimi2.5
    else if (taskType === 'creative' || taskType === 'writing') {
      console.log('ğŸ¨ ä½¿ç”¨ Kimi2.5 è¿›è¡Œåˆ›æ„ç”Ÿæˆ');
      return this.callKimi25(messages, { thinking: true, ...options });
    }
    // ä¸­ç­‰å¤æ‚åº¦æ¨ç† - ä½¿ç”¨GLM4.7
    else if (taskType === 'conversation' && messageLength > 5000) {
      console.log('ğŸ’­ ä½¿ç”¨ GLM4.7 è¿›è¡Œæ€ç»´æ¨ç†');
      return this.callGLM47(messages, { enableThinking: true, ...options });
    }
    // ç®€å•å¯¹è¯ - ä½¿ç”¨GLM4.7å¿«é€Ÿæ¨¡å¼
    else {
      console.log('ğŸ’¬ ä½¿ç”¨ GLM4.7 è¿›è¡Œå¿«é€Ÿå¯¹è¯');
      return this.callGLM47(messages, { enableThinking: false, ...options });
    }
  }

  // ä¸“ä¸šå•†ä¸šåˆ†æè°ƒç”¨
  async businessAnalysis(messages: Message[], options: CallOptions = {}): Promise<any> {
    console.log('ğŸ“Š ä½¿ç”¨ Nemotron è¿›è¡Œä¸“ä¸šå•†ä¸šåˆ†æ');
    return this.callNemotron(messages, {
      temperature: 0.7,
      maxTokens: 2048,
      enableThinking: true,
      reasoningBudget: 1024,
      ...options
    });
  }

  // è§†è§‰æ¨¡å‹è°ƒç”¨ (æ”¯æŒå›¾ç‰‡åˆ†æ)
  async callVisionModel(messages: Message[], options: CallOptions = {}): Promise<any> {
    const {
      temperature = 0.3,
      maxTokens = 1000
    } = options;

    const payload = {
      model: 'meta/llama-3.2-11b-vision-instruct',
      messages,
      temperature,
      top_p: 0.9,
      max_tokens: maxTokens,
      stream: false
    };

    try {
      const response = await fetch(`${this.baseUrl}/chat/completions`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json',
          'User-Agent': 'Piaoshu-Agent/1.0'
        },
        body: JSON.stringify(payload)
      });

      if (!response.ok) {
        const errorText = await response.text();
        console.error('Vision model error:', response.status, errorText);
        throw new Error(`Vision model API error: ${response.status} - ${errorText}`);
      }

      return await response.json();
    } catch (error) {
      console.error('Vision model call failed:', error);
      throw error;
    }
  }

  // è¿æ¥æµ‹è¯•
  async checkConnection(): Promise<boolean> {
    try {
      const response = await this.callGLM47([
        { role: 'user', content: 'Hello' }
      ], { maxTokens: 10 });
      
      return response.choices?.[0]?.message?.content ? true : false;
    } catch (error) {
      console.error('NVIDIA API connection failed:', error);
      return false;
    }
  }

  // è·å–å¯ç”¨æ¨¡å‹ä¿¡æ¯
  getAvailableModels(): any[] {
    return [
      {
        model: 'z-ai/glm4.7',
        name: 'GLM-4.7B',
        description: 'æ™ºè°±AI GLM-4.7Bï¼Œæ”¯æŒæ€ç»´é“¾æ¨ç†ï¼Œé€‚åˆå¯¹è¯å’Œä¸­ç­‰å¤æ‚åº¦åˆ†æ',
        maxTokens: 16384,
        supportThinking: true,
        bestFor: ['conversation', 'general_reasoning']
      },
      {
        model: 'moonshotai/kimi-k2.5',
        name: 'Kimi-K2.5',
        description: 'Moonshot Kimi-K2.5ï¼Œæ”¯æŒé•¿æ–‡æœ¬å’Œåˆ›æ„ç”Ÿæˆ',
        maxTokens: 16384,
        supportThinking: true,
        bestFor: ['creative', 'writing', 'long_text']
      },
      {
        model: 'nvidia/nemotron-3-nano-30b-a3b',
        name: 'Nemotron-3-Nano-30B',
        description: 'NVIDIA Nemotron 30Bå‚æ•°æ¨ç†ä¸“ç”¨æ¨¡å‹ï¼Œé€‚åˆå¤æ‚åˆ†æå’Œå•†ä¸šå†³ç­–',
        maxTokens: 16384,
        supportThinking: true,
        supportReasoning: true,
        bestFor: ['reasoning', 'analysis', 'business_decision']
      }
    ];
  }

  // æ¨¡å‹æ€§èƒ½ç»Ÿè®¡
  getModelStats(): any {
    return {
      'GLM4.7': {
        avgResponseTime: '13s',
        complexity: 'medium',
        reliability: 'high'
      },
      'Kimi2.5': {
        avgResponseTime: '75s',
        complexity: 'high',
        reliability: 'medium'
      },
      'Nemotron': {
        avgResponseTime: '15s',
        complexity: 'high',
        reliability: 'high'
      }
    };
  }
}