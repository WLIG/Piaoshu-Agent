// NVIDIA Build API å¤šæ¨¡å‹é›†æˆ
// æ”¯æŒ GLM4.7 å’Œ Kimi2.5 ç­‰å¤šç§å¤§æ¨¡å‹

export interface NvidiaModelConfig {
  model: string;
  name: string;
  description: string;
  maxTokens: number;
  supportThinking: boolean;
  supportVision: boolean;
}

export const NVIDIA_MODELS: { [key: string]: NvidiaModelConfig } = {
  'glm4.7': {
    model: 'z-ai/glm4.7',
    name: 'GLM-4.7B',
    description: 'æ™ºè°±AI GLM-4.7Bï¼Œæ”¯æŒæ€ç»´é“¾æ¨ç†',
    maxTokens: 16384,
    supportThinking: true,
    supportVision: false
  },
  'kimi2.5': {
    model: 'moonshotai/kimi-k2.5',
    name: 'Kimi-K2.5',
    description: 'Moonshot Kimi-K2.5ï¼Œæ”¯æŒé•¿æ–‡æœ¬å’Œæ€ç»´æ¨ç†',
    maxTokens: 16384,
    supportThinking: true,
    supportVision: true
  }
};

export interface NvidiaMessage {
  role: 'user' | 'assistant' | 'system';
  content: string;
}

export interface NvidiaResponse {
  id: string;
  object: string;
  created: number;
  model: string;
  choices: Array<{
    index: number;
    message: {
      role: string;
      content: string;
      reasoning_content?: string;
    };
    finish_reason: string;
  }>;
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

export class NvidiaModelClient {
  private apiKey: string;
  private baseUrl: string;

  constructor() {
    this.apiKey = process.env.NVIDIA_API_KEY || '';
    this.baseUrl = process.env.NVIDIA_BASE_URL || 'https://integrate.api.nvidia.com/v1';
    
    if (!this.apiKey) {
      throw new Error('NVIDIA_API_KEY is required');
    }
  }

  // GLM4.7 è°ƒç”¨ - æ”¯æŒæ€ç»´é“¾æ¨ç†
  async callGLM47(
    messages: NvidiaMessage[],
    options: {
      temperature?: number;
      topP?: number;
      maxTokens?: number;
      enableThinking?: boolean;
      clearThinking?: boolean;
      stream?: boolean;
    } = {}
  ): Promise<NvidiaResponse> {
    const {
      temperature = 0.7,
      topP = 0.9,
      maxTokens = 4096,
      enableThinking = true,
      clearThinking = false,
      stream = false
    } = options;

    const payload = {
      model: NVIDIA_MODELS.glm4.7.model,
      messages,
      temperature,
      top_p: topP,
      max_tokens: maxTokens,
      stream,
      extra_body: {
        chat_template_kwargs: {
          enable_thinking: enableThinking,
          clear_thinking: clearThinking
        }
      }
    };

    const response = await fetch(`${this.baseUrl}/chat/completions`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.apiKey}`,
        'Content-Type': 'application/json',
        'Accept': stream ? 'text/event-stream' : 'application/json'
      },
      body: JSON.stringify(payload)
    });

    if (!response.ok) {
      throw new Error(`GLM4.7 API error: ${response.status} ${response.statusText}`);
    }

    if (stream) {
      return this.handleStreamResponse(response);
    }

    return await response.json();
  }

  // Kimi2.5 è°ƒç”¨ - æ”¯æŒé•¿æ–‡æœ¬å’Œè§†è§‰
  async callKimi25(
    messages: NvidiaMessage[],
    options: {
      temperature?: number;
      topP?: number;
      maxTokens?: number;
      thinking?: boolean;
      stream?: boolean;
    } = {}
  ): Promise<NvidiaResponse> {
    const {
      temperature = 0.8,
      topP = 0.95,
      maxTokens = 8192,
      thinking = true,
      stream = false
    } = options;

    const payload = {
      model: NVIDIA_MODELS['kimi2.5'].model,
      messages,
      temperature,
      top_p: topP,
      max_tokens: maxTokens,
      stream,
      chat_template_kwargs: {
        thinking
      }
    };

    const response = await fetch(`${this.baseUrl}/chat/completions`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.apiKey}`,
        'Content-Type': 'application/json',
        'Accept': stream ? 'text/event-stream' : 'application/json'
      },
      body: JSON.stringify(payload)
    });

    if (!response.ok) {
      throw new Error(`Kimi2.5 API error: ${response.status} ${response.statusText}`);
    }

    if (stream) {
      return this.handleStreamResponse(response);
    }

    return await response.json();
  }

  // æ™ºèƒ½æ¨¡å‹é€‰æ‹© - æ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹
  async smartCall(
    messages: NvidiaMessage[],
    taskType: 'reasoning' | 'creative' | 'analysis' | 'conversation' = 'conversation',
    options: any = {}
  ): Promise<NvidiaResponse> {
    const messageLength = messages.reduce((sum: number, msg: NvidiaMessage) => sum + msg.content.length, 0);
    
    // æ ¹æ®ä»»åŠ¡ç±»å‹å’Œæ¶ˆæ¯é•¿åº¦æ™ºèƒ½é€‰æ‹©æ¨¡å‹
    if (taskType === 'reasoning' || messageLength > 8000) {
      console.log('ğŸ§  ä½¿ç”¨ GLM4.7 è¿›è¡Œæ¨ç†ä»»åŠ¡');
      return this.callGLM47(messages, {
        enableThinking: true,
        ...options
      });
    } else if (taskType === 'creative' || taskType === 'analysis') {
      console.log('ğŸ¨ ä½¿ç”¨ Kimi2.5 è¿›è¡Œåˆ›æ„/åˆ†æä»»åŠ¡');
      return this.callKimi25(messages, {
        thinking: true,
        ...options
      });
    } else {
      // é»˜è®¤ä½¿ç”¨GLM4.7è¿›è¡Œå¯¹è¯
      console.log('ğŸ’¬ ä½¿ç”¨ GLM4.7 è¿›è¡Œå¯¹è¯');
      return this.callGLM47(messages, {
        enableThinking: false,
        ...options
      });
    }
  }

  // å¤„ç†æµå¼å“åº”
  private async handleStreamResponse(response: Response): Promise<NvidiaResponse> {
    const reader = response.body?.getReader();
    if (!reader) {
      throw new Error('No response body');
    }

    let fullContent = '';
    let reasoningContent = '';
    
    try {
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = new TextDecoder().decode(value);
        const lines = chunk.split('\n');

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6);
            if (data === '[DONE]') continue;

            try {
              const parsed = JSON.parse(data);
              const delta = parsed.choices?.[0]?.delta;
              
              if (delta?.reasoning_content) {
                reasoningContent += delta.reasoning_content;
              }
              
              if (delta?.content) {
                fullContent += delta.content;
              }
            } catch (e) {
              // å¿½ç•¥è§£æé”™è¯¯
            }
          }
        }
      }
    } finally {
      reader.releaseLock();
    }

    // è¿”å›æ¨¡æ‹Ÿçš„å®Œæ•´å“åº”
    return {
      id: `nvidia-${Date.now()}`,
      object: 'chat.completion',
      created: Math.floor(Date.now() / 1000),
      model: 'nvidia-model',
      choices: [{
        index: 0,
        message: {
          role: 'assistant',
          content: fullContent,
          reasoning_content: reasoningContent
        },
        finish_reason: 'stop'
      }],
      usage: {
        prompt_tokens: 0,
        completion_tokens: 0,
        total_tokens: 0
      }
    };
  }

  // è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
  getAvailableModels(): NvidiaModelConfig[] {
    return Object.values(NVIDIA_MODELS);
  }

  // æ£€æŸ¥APIè¿æ¥çŠ¶æ€
  async checkConnection(): Promise<boolean> {
    try {
      const response = await this.callGLM47([
        { role: 'user', content: 'Hello' }
      ], { maxTokens: 10 });
      
      return response.choices?.[0]?.message?.content ? true : false;
    } catch (error) {
      console.error('NVIDIA API connection failed:', error);
      return false;
    }
  }
}