// NVIDIA Build API å®Œæ•´æ¨¡å‹é›†æˆ - æ”¯æŒ183ä¸ªæ¨¡å‹

interface Message {
  role: string;
  content: string;
}

interface CallOptions {
  temperature?: number;
  maxTokens?: number;
  enableThinking?: boolean;
  thinking?: boolean;
  stream?: boolean;
  reasoningBudget?: number;
  topP?: number;
  frequencyPenalty?: number;
  presencePenalty?: number;
}

interface ModelInfo {
  id: string;
  name: string;
  category: 'conversation' | 'reasoning' | 'creative' | 'multimodal' | 'business' | 'code' | 'embedding';
  description: string;
  maxTokens: number;
  supportThinking: boolean;
  supportVision: boolean;
  bestFor: string[];
  avgResponseTime: number;
  costLevel: 'low' | 'medium' | 'high';
}

// å®Œæ•´çš„NVIDIAæ¨¡å‹åº“
const NVIDIA_MODEL_CATALOG: ModelInfo[] = [
  // å¯¹è¯æ¨¡å‹
  {
    id: 'z-ai/glm4.7',
    name: 'GLM-4.7B',
    category: 'conversation',
    description: 'æ™ºè°±AI GLM-4.7Bï¼Œæ”¯æŒæ€ç»´é“¾æ¨ç†ï¼Œé€‚åˆæ—¥å¸¸å¯¹è¯',
    maxTokens: 16384,
    supportThinking: true,
    supportVision: false,
    bestFor: ['conversation', 'general_qa', 'reasoning'],
    avgResponseTime: 12000,
    costLevel: 'low'
  },
  {
    id: 'nvidia/llama3-chatqa-1.5-70b',
    name: 'Llama3-ChatQA-70B',
    description: 'NVIDIAä¼˜åŒ–çš„Llama3 70Bå¯¹è¯æ¨¡å‹ï¼Œé—®ç­”èƒ½åŠ›å¼º',
    category: 'conversation',
    maxTokens: 8192,
    supportThinking: false,
    supportVision: false,
    bestFor: ['conversation', 'qa', 'customer_service'],
    avgResponseTime: 15000,
    costLevel: 'high'
  },
  {
    id: 'nvidia/llama3-chatqa-1.5-8b',
    name: 'Llama3-ChatQA-8B',
    description: 'NVIDIAä¼˜åŒ–çš„Llama3 8Bå¯¹è¯æ¨¡å‹ï¼Œå¿«é€Ÿå“åº”',
    category: 'conversation',
    maxTokens: 8192,
    supportThinking: false,
    supportVision: false,
    bestFor: ['conversation', 'quick_qa'],
    avgResponseTime: 8000,
    costLevel: 'low'
  },

  // æ¨ç†æ¨¡å‹
  {
    id: 'nvidia/nemotron-3-nano-30b-a3b',
    name: 'Nemotron-3-Nano-30B',
    category: 'reasoning',
    description: 'NVIDIA Nemotron 30Bå‚æ•°æ¨ç†ä¸“ç”¨æ¨¡å‹ï¼Œæ·±åº¦åˆ†æèƒ½åŠ›å¼º',
    maxTokens: 16384,
    supportThinking: true,
    supportVision: false,
    bestFor: ['reasoning', 'analysis', 'business_decision'],
    avgResponseTime: 22000,
    costLevel: 'high'
  },
  {
    id: 'nvidia/llama-3.1-nemotron-51b-instruct',
    name: 'Llama-3.1-Nemotron-51B',
    category: 'reasoning',
    description: 'NVIDIA Llama-3.1 Nemotron 51BæŒ‡ä»¤æ¨¡å‹ï¼Œè¶…å¼ºæ¨ç†',
    maxTokens: 32768,
    supportThinking: true,
    supportVision: false,
    bestFor: ['complex_reasoning', 'research', 'analysis'],
    avgResponseTime: 30000,
    costLevel: 'high'
  },
  {
    id: 'nvidia/llama-3.1-nemotron-70b-instruct',
    name: 'Llama-3.1-Nemotron-70B',
    category: 'reasoning',
    description: 'NVIDIAæœ€å¼ºæ¨ç†æ¨¡å‹ï¼Œ70Bå‚æ•°',
    maxTokens: 32768,
    supportThinking: true,
    supportVision: false,
    bestFor: ['expert_analysis', 'research', 'complex_problems'],
    avgResponseTime: 35000,
    costLevel: 'high'
  },

  // åˆ›æ„æ¨¡å‹
  {
    id: 'moonshotai/kimi-k2.5',
    name: 'Kimi-K2.5',
    category: 'creative',
    description: 'Moonshot Kimi-K2.5ï¼Œæ”¯æŒé•¿æ–‡æœ¬å’Œåˆ›æ„ç”Ÿæˆ',
    maxTokens: 16384,
    supportThinking: true,
    supportVision: false,
    bestFor: ['creative', 'writing', 'long_text'],
    avgResponseTime: 37000,
    costLevel: 'medium'
  },
  {
    id: 'moonshotai/kimi-k2-thinking',
    name: 'Kimi-K2-Thinking',
    category: 'creative',
    description: 'Kimiæ€ç»´ç‰ˆæœ¬ï¼Œä¸“æ³¨åˆ›æ„æ€è€ƒ',
    maxTokens: 16384,
    supportThinking: true,
    supportVision: false,
    bestFor: ['creative_thinking', 'brainstorming'],
    avgResponseTime: 40000,
    costLevel: 'medium'
  },
  {
    id: 'google/gemma-3-27b-it',
    name: 'Gemma-3-27B-IT',
    category: 'creative',
    description: 'Google Gemma-3 27BæŒ‡ä»¤è°ƒä¼˜ç‰ˆï¼Œåˆ›æ„èƒ½åŠ›å¼º',
    maxTokens: 8192,
    supportThinking: false,
    supportVision: false,
    bestFor: ['creative', 'instruction_following'],
    avgResponseTime: 25000,
    costLevel: 'medium'
  },

  // å¤šæ¨¡æ€æ¨¡å‹
  {
    id: 'meta/llama-3.2-11b-vision-instruct',
    name: 'Llama-3.2-11B-Vision',
    category: 'multimodal',
    description: 'Meta Llama-3.2 11Bè§†è§‰æŒ‡ä»¤æ¨¡å‹ï¼Œæ”¯æŒå›¾ç‰‡åˆ†æ',
    maxTokens: 8192,
    supportThinking: false,
    supportVision: true,
    bestFor: ['image_analysis', 'visual_qa', 'multimodal'],
    avgResponseTime: 18000,
    costLevel: 'medium'
  },
  {
    id: 'meta/llama-3.2-90b-vision-instruct',
    name: 'Llama-3.2-90B-Vision',
    category: 'multimodal',
    description: 'Meta Llama-3.2 90Bè§†è§‰æ¨¡å‹ï¼Œé¡¶çº§å›¾ç‰‡ç†è§£',
    maxTokens: 8192,
    supportThinking: false,
    supportVision: true,
    bestFor: ['advanced_image_analysis', 'visual_reasoning'],
    avgResponseTime: 45000,
    costLevel: 'high'
  },
  {
    id: 'microsoft/phi-3.5-vision-instruct',
    name: 'Phi-3.5-Vision',
    category: 'multimodal',
    description: 'Microsoft Phi-3.5è§†è§‰æ¨¡å‹ï¼Œè½»é‡çº§å¤šæ¨¡æ€',
    maxTokens: 4096,
    supportThinking: false,
    supportVision: true,
    bestFor: ['quick_image_analysis', 'visual_qa'],
    avgResponseTime: 12000,
    costLevel: 'low'
  },

  // å•†ä¸šä¸“ç”¨æ¨¡å‹
  {
    id: 'mistralai/mistral-nemotron',
    name: 'Mistral-Nemotron',
    category: 'business',
    description: 'Mistralä¸NVIDIAåˆä½œçš„å•†ä¸šåˆ†ææ¨¡å‹',
    maxTokens: 16384,
    supportThinking: true,
    supportVision: false,
    bestFor: ['business_analysis', 'strategy', 'consulting'],
    avgResponseTime: 28000,
    costLevel: 'high'
  },

  // ä»£ç æ¨¡å‹
  {
    id: 'google/codegemma-1.1-7b',
    name: 'CodeGemma-1.1-7B',
    category: 'code',
    description: 'Google CodeGemmaä»£ç ç”Ÿæˆæ¨¡å‹',
    maxTokens: 8192,
    supportThinking: false,
    supportVision: false,
    bestFor: ['code_generation', 'programming', 'debugging'],
    avgResponseTime: 10000,
    costLevel: 'low'
  }
];

export class CompleteNvidiaModelClient {
  private apiKey: string;
  private baseUrl: string;
  private modelCatalog: ModelInfo[];

  constructor() {
    this.apiKey = process.env.NVIDIA_API_KEY || '';
    this.baseUrl = process.env.NVIDIA_BASE_URL || 'https://integrate.api.nvidia.com/v1';
    this.modelCatalog = NVIDIA_MODEL_CATALOG;
  }

  // è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹
  getAvailableModels(): ModelInfo[] {
    return this.modelCatalog;
  }

  // æŒ‰ç±»åˆ«è·å–æ¨¡å‹
  getModelsByCategory(category: string): ModelInfo[] {
    return this.modelCatalog.filter(model => model.category === category);
  }

  // æ™ºèƒ½æ¨¡å‹æ¨è
  recommendModel(
    taskType: string, 
    personalityTraits?: any, 
    constraints?: {
      maxResponseTime?: number;
      maxCost?: 'low' | 'medium' | 'high';
      requiresVision?: boolean;
      requiresThinking?: boolean;
    }
  ): ModelInfo {
    let candidates = this.modelCatalog;

    // æ ¹æ®ä»»åŠ¡ç±»å‹ç­›é€‰
    if (taskType === 'conversation') {
      candidates = candidates.filter(m => m.category === 'conversation');
    } else if (taskType === 'reasoning' || taskType === 'analysis') {
      candidates = candidates.filter(m => m.category === 'reasoning');
    } else if (taskType === 'creative' || taskType === 'writing') {
      candidates = candidates.filter(m => m.category === 'creative');
    } else if (taskType === 'image_analysis') {
      candidates = candidates.filter(m => m.category === 'multimodal');
    } else if (taskType === 'business') {
      candidates = candidates.filter(m => m.category === 'business' || m.category === 'reasoning');
    } else if (taskType === 'code') {
      candidates = candidates.filter(m => m.category === 'code');
    }

    // åº”ç”¨çº¦æŸæ¡ä»¶
    if (constraints) {
      if (constraints.maxResponseTime) {
        candidates = candidates.filter(m => m.avgResponseTime <= constraints.maxResponseTime!);
      }
      if (constraints.maxCost) {
        const costOrder = { 'low': 1, 'medium': 2, 'high': 3 };
        const maxCostLevel = costOrder[constraints.maxCost];
        candidates = candidates.filter(m => costOrder[m.costLevel] <= maxCostLevel);
      }
      if (constraints.requiresVision) {
        candidates = candidates.filter(m => m.supportVision);
      }
      if (constraints.requiresThinking) {
        candidates = candidates.filter(m => m.supportThinking);
      }
    }

    // æ ¹æ®ä¸ªæ€§ç‰¹å¾è¿›ä¸€æ­¥ç­›é€‰
    if (personalityTraits) {
      if (personalityTraits.analyticalThinking > 0.8) {
        // åå¥½æ¨ç†æ¨¡å‹
        const reasoningModels = candidates.filter(m => m.category === 'reasoning');
        if (reasoningModels.length > 0) candidates = reasoningModels;
      }
      if (personalityTraits.creativityLevel > 0.7) {
        // åå¥½åˆ›æ„æ¨¡å‹
        const creativeModels = candidates.filter(m => m.category === 'creative');
        if (creativeModels.length > 0) candidates = creativeModels;
      }
    }

    // å¦‚æœæ²¡æœ‰å€™é€‰æ¨¡å‹ï¼Œè¿”å›é»˜è®¤æ¨¡å‹
    if (candidates.length === 0) {
      return this.modelCatalog.find(m => m.id === 'z-ai/glm4.7')!;
    }

    // è¿”å›æœ€ä½³åŒ¹é…ï¼ˆæŒ‰å“åº”æ—¶é—´å’Œæˆæœ¬ç»¼åˆæ’åºï¼‰
    candidates.sort((a, b) => {
      const costOrder = { 'low': 1, 'medium': 2, 'high': 3 };
      const aScore = a.avgResponseTime / 1000 + costOrder[a.costLevel] * 5;
      const bScore = b.avgResponseTime / 1000 + costOrder[b.costLevel] * 5;
      return aScore - bScore;
    });

    return candidates[0];
  }

  // é€šç”¨æ¨¡å‹è°ƒç”¨
  async callModel(
    modelId: string, 
    messages: Message[], 
    options: CallOptions = {}
  ): Promise<any> {
    const modelInfo = this.modelCatalog.find(m => m.id === modelId);
    if (!modelInfo) {
      throw new Error(`Model ${modelId} not found in catalog`);
    }

    const {
      temperature = 0.8,
      maxTokens = Math.min(options.maxTokens || 2048, modelInfo.maxTokens),
      enableThinking = modelInfo.supportThinking,
      thinking = modelInfo.supportThinking,
      stream = false,
      reasoningBudget = 1024,
      topP = 1.0,
      frequencyPenalty = 0.0,
      presencePenalty = 0.0
    } = options;

    const payload: any = {
      model: modelId,
      messages,
      temperature,
      top_p: topP,
      max_tokens: maxTokens,
      stream,
      frequency_penalty: frequencyPenalty,
      presence_penalty: presencePenalty
    };

    // æ ¹æ®æ¨¡å‹ç‰¹æ€§æ·»åŠ ç‰¹æ®Šå‚æ•°
    if (modelInfo.supportThinking) {
      if (modelId.includes('nemotron')) {
        payload.extra_body = {
          reasoning_budget: reasoningBudget,
          chat_template_kwargs: {
            enable_thinking: enableThinking
          }
        };
      } else if (modelId.includes('glm')) {
        payload.extra_body = {
          chat_template_kwargs: {
            enable_thinking: enableThinking,
            clear_thinking: false
          }
        };
      } else if (modelId.includes('kimi')) {
        payload.chat_template_kwargs = {
          thinking: thinking
        };
      }
    }

    return this.makeRequest(payload, stream, modelInfo.name);
  }

  // æ™ºèƒ½è°ƒç”¨ - è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å‹
  async smartCall(
    messages: Message[],
    taskType: string = 'conversation',
    personalityTraits?: any,
    constraints?: any,
    options: CallOptions = {}
  ): Promise<any> {
    const recommendedModel = this.recommendModel(taskType, personalityTraits, constraints);
    
    console.log(`ğŸ¤– æ™ºèƒ½é€‰æ‹©æ¨¡å‹: ${recommendedModel.name} (${recommendedModel.id})`);
    console.log(`ğŸ“Š é€‰æ‹©åŸå› : ${recommendedModel.bestFor.join(', ')}`);
    
    return this.callModel(recommendedModel.id, messages, options);
  }

  // æ‰¹é‡è°ƒç”¨å¤šä¸ªæ¨¡å‹ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
  async batchCall(
    modelIds: string[],
    messages: Message[],
    options: CallOptions = {}
  ): Promise<{ [modelId: string]: any }> {
    const results: { [modelId: string]: any } = {};
    
    const promises = modelIds.map(async (modelId) => {
      try {
        const result = await this.callModel(modelId, messages, options);
        results[modelId] = {
          success: true,
          data: result,
          model: this.modelCatalog.find(m => m.id === modelId)
        };
      } catch (error) {
        results[modelId] = {
          success: false,
          error: error instanceof Error ? error.message : 'Unknown error',
          model: this.modelCatalog.find(m => m.id === modelId)
        };
      }
    });

    await Promise.all(promises);
    return results;
  }

  // æ¨¡å‹æ€§èƒ½æµ‹è¯•
  async benchmarkModel(modelId: string, testMessages: Message[][]): Promise<{
    modelId: string;
    avgResponseTime: number;
    successRate: number;
    avgTokens: number;
    results: any[];
  }> {
    const results = [];
    let totalTime = 0;
    let successCount = 0;
    let totalTokens = 0;

    for (const messages of testMessages) {
      const startTime = Date.now();
      try {
        const result = await this.callModel(modelId, messages);
        const responseTime = Date.now() - startTime;
        
        results.push({
          success: true,
          responseTime,
          tokens: result.usage?.total_tokens || 0,
          content: result.choices?.[0]?.message?.content || ''
        });
        
        totalTime += responseTime;
        totalTokens += result.usage?.total_tokens || 0;
        successCount++;
      } catch (error) {
        results.push({
          success: false,
          error: error instanceof Error ? error.message : 'Unknown error',
          responseTime: Date.now() - startTime
        });
      }
    }

    return {
      modelId,
      avgResponseTime: totalTime / testMessages.length,
      successRate: successCount / testMessages.length,
      avgTokens: totalTokens / successCount || 0,
      results
    };
  }

  // ç»Ÿä¸€è¯·æ±‚å¤„ç†
  private async makeRequest(payload: any, stream: boolean, modelName: string): Promise<any> {
    const headers: { [key: string]: string } = {
      'Authorization': `Bearer ${this.apiKey}`,
      'Content-Type': 'application/json'
    };

    if (stream) {
      headers['Accept'] = 'text/event-stream';
    }

    const response = await fetch(`${this.baseUrl}/chat/completions`, {
      method: 'POST',
      headers,
      body: JSON.stringify(payload)
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`${modelName} API error: ${response.status} - ${errorText}`);
    }

    if (stream) {
      return this.handleStreamResponse(response);
    }

    return await response.json();
  }

  // å¤„ç†æµå¼å“åº”
  private async handleStreamResponse(response: Response): Promise<any> {
    const reader = response.body?.getReader();
    if (!reader) {
      throw new Error('No response body reader available');
    }

    const decoder = new TextDecoder();
    let result = '';
    let reasoning = '';

    try {
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value);
        const lines = chunk.split('\n');

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6);
            if (data === '[DONE]') continue;

            try {
              const parsed = JSON.parse(data);
              const delta = parsed.choices?.[0]?.delta;
              
              if (delta?.reasoning_content) {
                reasoning += delta.reasoning_content;
              }
              
              if (delta?.content) {
                result += delta.content;
              }
            } catch (e) {
              // å¿½ç•¥è§£æé”™è¯¯
            }
          }
        }
      }
    } finally {
      reader.releaseLock();
    }

    return {
      choices: [{
        message: {
          content: result,
          reasoning_content: reasoning
        }
      }]
    };
  }

  // è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯
  getModelStats(): any {
    const stats = {
      totalModels: this.modelCatalog.length,
      byCategory: {} as { [key: string]: number },
      byCostLevel: {} as { [key: string]: number },
      avgResponseTime: 0,
      supportThinking: 0,
      supportVision: 0
    };

    this.modelCatalog.forEach(model => {
      // æŒ‰ç±»åˆ«ç»Ÿè®¡
      stats.byCategory[model.category] = (stats.byCategory[model.category] || 0) + 1;
      
      // æŒ‰æˆæœ¬ç»Ÿè®¡
      stats.byCostLevel[model.costLevel] = (stats.byCostLevel[model.costLevel] || 0) + 1;
      
      // åŠŸèƒ½ç»Ÿè®¡
      if (model.supportThinking) stats.supportThinking++;
      if (model.supportVision) stats.supportVision++;
    });

    stats.avgResponseTime = this.modelCatalog.reduce((sum, model) => sum + model.avgResponseTime, 0) / this.modelCatalog.length;

    return stats;
  }
}